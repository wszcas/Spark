{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import  SparkContext\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removePunctuation_tokenize(text):\n",
    "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces, then split by white space\n",
    "\n",
    "    Note:\n",
    "        Only whitespace, letters, and numbers should be retained.  Other characters should should be\n",
    "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
    "        punctuation is removed.\n",
    "\n",
    "    Args:\n",
    "        text (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned up string.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^A-Za-z\\s\\d]',r'',text).strip().lower().split()\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "removePunctuation_tokenize(\"   I lobe's tijasj  kkk!!\")  ## test the removePunctuation_tokenize function to make sure it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## read in three partial books file as well the query text from S3, load into rdd.  Query are texts taken from the book RiderHaggard_SheandAllan with alterations, including words skip and words typos.  \n",
    "r1 = requests.get('https://s3-us-west-1.amazonaws.com/datasw/GeorgeJames_WhiteRace.txt')\n",
    "r2 = requests.get('https://s3-us-west-1.amazonaws.com/datasw/RiderHaggard_SheandAllan.txt')\n",
    "r3 = requests.get('https://s3-us-west-1.amazonaws.com/datasw/StanleyMatthews_DoubleTrouble.txt')\n",
    "q=requests.get('https://s3-us-west-1.amazonaws.com/datasw/Query.txt')\n",
    "Book_GJ=[line for line in r1.iter_lines()]\n",
    "Book_RH=[line for line in r2.iter_lines()]\n",
    "Book_SM=[line for line in r3.iter_lines()]\n",
    "query=[line for line in q.iter_lines()]\n",
    "BookGJ_RDD = sc.parallelize(Book_GJ)\n",
    "BookRH_RDD = sc.parallelize(Book_RH)\n",
    "BookSM_RDD = sc.parallelize(Book_SM)\n",
    "Q_RDD=sc.parallelize(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BookGJ_RDD.take(5)\n",
    "BookRH_RDD.take(5)\n",
    "BookSM_RDD.take(5)\n",
    "Q_RDD.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## transform each book RDD using removePunctuation_tokenize function\n",
    "BookGJ_RDD_transformed=BookGJ_RDD.flatMap(removePunctuation_tokenize)\n",
    "BookRH_RDD_transformed=BookRH_RDD.flatMap(removePunctuation_tokenize)\n",
    "BookSM_RDD_transformed=BookSM_RDD.flatMap(removePunctuation_tokenize)\n",
    "Q_RDD_transformed=Q_RDD.flatMap(removePunctuation_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BookGJ_RDD_transformed.count()\n",
    "BookRH_RDD_transformed.count()\n",
    "BookSM_RDD_transformed.count()\n",
    "Q_RDD_transformed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "BookGJ_RDD_transformed.take(10)\n",
    "BookRH_RDD_transformed.take(10)\n",
    "BookSM_RDD_transformed.take(10)\n",
    "Q_RDD_transformed.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  now we have both books RDD and query RDD ready,  we need build a ngram for each of them, in order to trace which book the query is taken from.  We do have noises in the query, so the idea here is to find the book sources that have the maxium count of ngram matching with the query.  I used trigram here.\n",
    "\n",
    "input_list = ['all', 'this', 'happened', 'more', 'or', 'less']\n",
    "\n",
    "def find_trigrams(input_list):\n",
    "  return zip(input_list, input_list[1:],input_list[2:])\n",
    "\n",
    "find_trigrams(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BookGJ_trigram=find_trigrams([x for x in BookGJ_RDD_transformed.toLocalIterator()])\n",
    "BookRH_trigram=find_trigrams([x for x in BookRH_RDD_transformed.toLocalIterator()])\n",
    "BookSM_trigram=find_trigrams([x for x in BookSM_RDD_transformed.toLocalIterator()])\n",
    "Q_trigram=find_trigrams([x for x in Q_RDD_transformed.toLocalIterator()])\n",
    "#print BookGJ_trigram\n",
    "print Q_trigram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### now find the overlap of the ngram between query and the book sources\n",
    "\n",
    "def Overlap_count(query,book):\n",
    "  return(len([x for x in query if x in book]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Query Trigrams found \\n in Book_GJ is : %d \\n in Book_SM is %d \\n in Book_RH is %d'%(Overlap_count(Q_trigram,BookGJ_trigram),Overlap_count(Q_trigram,BookSM_trigram),Overlap_count(Q_trigram,BookRH_trigram))\n",
    "\n",
    "\n",
    "## we can see it makes correct prediction, the query is taken from book RH(RiderHaggard_SheandAllan) with alterations, including words skip and words typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### now we start working a separate task, to predict the author of the query when the query was not seen before, so we read in a new text query extracted from the author RiderHaggard, but not included in the book RH(RiderHaggard_SheandAllan) we previously readed in\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "name": "Assignment",
  "notebookId": 473766496434885
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
